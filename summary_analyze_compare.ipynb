{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to analyze, benchmark, compare in pytorch ?\n",
    "\n",
    "To devleop new model in pytorch, Analyzing existing model is mandatory work to do. But I could not find any one-in-whole packages for these purpose. To analyze model, we need to see input/output, param, flops, latency and throughput for both in model level and in layer level. I spent a lot of time googling about this, these are the tools that I decide to use for each purpose. I hope this could help you.\n",
    "\n",
    "- Want to see model's structure in clean view? use `torchsummary`\n",
    "- Want to analyze model's latency/param/flops in layer? use `deepspeed.profiling.flops_profiler.get_model_profile()`. Other library seems not support these 3 important factors.\n",
    "    - `torch.profile`: This library supports latency/param/flops/memory, but in cuda kernel operation level. It seems to good for who want to optimize module in lower level.\n",
    "    - `fvcore.profile`: This library supports flops/param only. For me, it looks like upgraded version of `torchsummary`\n",
    "\n",
    "- Want to compare simply latency with others? use `torch.utils.benchmark`.\n",
    "    - Latency is `1/throughput`, which means that latency is affected by throughput factor such as batch size.\n",
    "- Want to compare overall performance(params/flops/latency/throughput)? use `benchmark.py` in `timm` github\n",
    "\n",
    "### Procedure to analyze model\n",
    "1. Check model works as you intended by using `torchsummary`\n",
    "2. Check model's param/flops/latency and which is bottleneck of your model by using profiler.\n",
    "3. Compare model with other baseline model.\n",
    "    - Find how to ease your bottleneck by comparing different method.\n",
    "4. Finally check your models' param/flops/latency/thoughput and start experiment on your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 25,557,032\n",
      "Trainable params: 25,557,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.56\n",
      "Params size (MB): 97.49\n",
      "Estimated Total Size (MB): 384.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "model = models.resnet50().cuda()\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Param/Flops/Latency in layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 10:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating point operations (flops), floating point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per gpu:                                               11.69 M \n",
      "params of model = params per GPU * mp_size:                   1       \n",
      "fwd MACs per GPU:                                             182.22 G\n",
      "fwd flops per GPU = 2 * fwd MACs per GPU:                     364.44 G\n",
      "fwd flops of model = fwd flops per GPU * mp_size:             1       \n",
      "fwd latency:                                                  8.52 ms \n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          42.75 TFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'ResNet': '11.69 M'}\n",
      "    MACs        - {'ResNet': '182.22 GMACs'}\n",
      "    fwd latency - {'ResNet': '8.52 ms'}\n",
      "depth 1:\n",
      "    params      - {'Sequential': '11.17 M'}\n",
      "    MACs        - {'Sequential': '170.04 GMACs'}\n",
      "    fwd latency - {'Sequential': '6.93 ms'}\n",
      "depth 2:\n",
      "    params      - {'BasicBlock': '11.17 M'}\n",
      "    MACs        - {'BasicBlock': '170.04 GMACs'}\n",
      "    fwd latency - {'BasicBlock': '6.85 ms'}\n",
      "depth 3:\n",
      "    params      - {'Conv2d': '10.99 M'}\n",
      "    MACs        - {'Conv2d': '167.63 GMACs'}\n",
      "    fwd latency - {'Conv2d': '3.6 ms'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "ResNet(\n",
      "  11.69 M, 100.00% Params, 182.22 GMACs, 100.00% MACs, 8.52 ms, 100.00% latency, 42.75 TFLOPS, \n",
      "  (conv1): Conv2d(9.41 k, 0.08% Params, 11.8 GMACs, 6.48% MACs, 203.13 us, 2.38% latency, 116.19 TFLOPS, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(128, 0.00% Params, 160.56 MMACs, 0.09% MACs, 65.57 us, 0.77% latency, 4.9 TFLOPS, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(0, 0.00% Params, 80.28 MMACs, 0.04% MACs, 32.42 us, 0.38% latency, 4.95 TFLOPS, inplace=True)\n",
      "  (maxpool): MaxPool2d(0, 0.00% Params, 80.28 MMACs, 0.04% MACs, 42.68 us, 0.50% latency, 3.76 TFLOPS, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    147.97 k, 1.27% Params, 46.48 GMACs, 25.51% MACs, 958.68 us, 11.25% latency, 96.97 TFLOPS, \n",
      "    (0): BasicBlock(\n",
      "      73.98 k, 0.63% Params, 23.24 GMACs, 12.75% MACs, 494.96 us, 5.81% latency, 93.91 TFLOPS, \n",
      "      (conv1): Conv2d(36.86 k, 0.32% Params, 11.56 GMACs, 6.34% MACs, 150.92 us, 1.77% latency, 153.2 TFLOPS, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, 0.00% Params, 40.14 MMACs, 0.02% MACs, 52.69 us, 0.62% latency, 1.52 TFLOPS, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.00% Params, 40.14 MMACs, 0.02% MACs, 51.02 us, 0.60% latency, 1.57 TFLOPS, inplace=True)\n",
      "      (conv2): Conv2d(36.86 k, 0.32% Params, 11.56 GMACs, 6.34% MACs, 122.07 us, 1.43% latency, 189.41 TFLOPS, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, 0.00% Params, 40.14 MMACs, 0.02% MACs, 47.68 us, 0.56% latency, 1.68 TFLOPS, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      73.98 k, 0.63% Params, 23.24 GMACs, 12.75% MACs, 444.65 us, 5.22% latency, 104.54 TFLOPS, \n",
      "      (conv1): Conv2d(36.86 k, 0.32% Params, 11.56 GMACs, 6.34% MACs, 118.26 us, 1.39% latency, 195.52 TFLOPS, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, 0.00% Params, 40.14 MMACs, 0.02% MACs, 48.64 us, 0.57% latency, 1.65 TFLOPS, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.00% Params, 40.14 MMACs, 0.02% MACs, 48.4 us, 0.57% latency, 1.66 TFLOPS, inplace=True)\n",
      "      (conv2): Conv2d(36.86 k, 0.32% Params, 11.56 GMACs, 6.34% MACs, 115.63 us, 1.36% latency, 199.95 TFLOPS, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, 0.00% Params, 40.14 MMACs, 0.02% MACs, 46.97 us, 0.55% latency, 1.71 TFLOPS, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    525.57 k, 4.50% Params, 41.24 GMACs, 22.63% MACs, 1.22 ms, 14.26% latency, 67.85 TFLOPS, \n",
      "    (0): BasicBlock(\n",
      "      230.14 k, 1.97% Params, 18.06 GMACs, 9.91% MACs, 716.69 us, 8.41% latency, 50.41 TFLOPS, \n",
      "      (conv1): Conv2d(73.73 k, 0.63% Params, 5.78 GMACs, 3.17% MACs, 149.97 us, 1.76% latency, 77.09 TFLOPS, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, 0.00% Params, 20.07 MMACs, 0.01% MACs, 50.78 us, 0.60% latency, 790.44 GFLOPS, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.00% Params, 20.07 MMACs, 0.01% MACs, 50.54 us, 0.59% latency, 794.16 GFLOPS, inplace=True)\n",
      "      (conv2): Conv2d(147.46 k, 1.26% Params, 11.56 GMACs, 6.34% MACs, 138.52 us, 1.62% latency, 166.91 TFLOPS, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, 0.00% Params, 20.07 MMACs, 0.01% MACs, 48.88 us, 0.57% latency, 821.28 GFLOPS, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        8.45 k, 0.07% Params, 662.32 MMACs, 0.36% MACs, 199.56 us, 2.34% latency, 6.64 TFLOPS, \n",
      "        (0): Conv2d(8.19 k, 0.07% Params, 642.25 MMACs, 0.35% MACs, 129.22 us, 1.52% latency, 9.94 TFLOPS, 64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, 0.00% Params, 20.07 MMACs, 0.01% MACs, 48.88 us, 0.57% latency, 821.28 GFLOPS, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      295.42 k, 2.53% Params, 23.18 GMACs, 12.72% MACs, 481.61 us, 5.65% latency, 96.27 TFLOPS, \n",
      "      (conv1): Conv2d(147.46 k, 1.26% Params, 11.56 GMACs, 6.34% MACs, 137.33 us, 1.61% latency, 168.36 TFLOPS, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, 0.00% Params, 20.07 MMACs, 0.01% MACs, 49.83 us, 0.58% latency, 805.56 GFLOPS, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.00% Params, 20.07 MMACs, 0.01% MACs, 49.35 us, 0.58% latency, 813.35 GFLOPS, inplace=True)\n",
      "      (conv2): Conv2d(147.46 k, 1.26% Params, 11.56 GMACs, 6.34% MACs, 127.79 us, 1.50% latency, 180.93 TFLOPS, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, 0.00% Params, 20.07 MMACs, 0.01% MACs, 49.59 us, 0.58% latency, 809.44 GFLOPS, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    2.1 M, 17.96% Params, 41.17 GMACs, 22.60% MACs, 1.17 ms, 13.77% latency, 70.13 TFLOPS, \n",
      "    (0): BasicBlock(\n",
      "      919.04 k, 7.86% Params, 18.02 GMACs, 9.89% MACs, 689.74 us, 8.09% latency, 52.26 TFLOPS, \n",
      "      (conv1): Conv2d(294.91 k, 2.52% Params, 5.78 GMACs, 3.17% MACs, 130.18 us, 1.53% latency, 88.81 TFLOPS, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, 0.00% Params, 10.04 MMACs, 0.01% MACs, 51.26 us, 0.60% latency, 391.54 GFLOPS, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.00% Params, 10.04 MMACs, 0.01% MACs, 49.11 us, 0.58% latency, 408.65 GFLOPS, inplace=True)\n",
      "      (conv2): Conv2d(589.82 k, 5.05% Params, 11.56 GMACs, 6.34% MACs, 131.85 us, 1.55% latency, 175.37 TFLOPS, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, 0.00% Params, 10.04 MMACs, 0.01% MACs, 49.35 us, 0.58% latency, 406.67 GFLOPS, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        33.28 k, 0.28% Params, 652.29 MMACs, 0.36% MACs, 200.03 us, 2.35% latency, 6.52 TFLOPS, \n",
      "        (0): Conv2d(32.77 k, 0.28% Params, 642.25 MMACs, 0.35% MACs, 129.22 us, 1.52% latency, 9.94 TFLOPS, 128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, 0.00% Params, 10.04 MMACs, 0.01% MACs, 49.59 us, 0.58% latency, 404.72 GFLOPS, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      1.18 M, 10.10% Params, 23.15 GMACs, 12.71% MACs, 466.11 us, 5.47% latency, 99.34 TFLOPS, \n",
      "      (conv1): Conv2d(589.82 k, 5.05% Params, 11.56 GMACs, 6.34% MACs, 129.94 us, 1.52% latency, 177.94 TFLOPS, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, 0.00% Params, 10.04 MMACs, 0.01% MACs, 48.16 us, 0.56% latency, 416.74 GFLOPS, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.00% Params, 10.04 MMACs, 0.01% MACs, 48.64 us, 0.57% latency, 412.65 GFLOPS, inplace=True)\n",
      "      (conv2): Conv2d(589.82 k, 5.05% Params, 11.56 GMACs, 6.34% MACs, 123.26 us, 1.45% latency, 187.58 TFLOPS, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, 0.00% Params, 10.04 MMACs, 0.01% MACs, 48.16 us, 0.56% latency, 416.74 GFLOPS, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    8.39 M, 71.81% Params, 41.14 GMACs, 22.58% MACs, 3.58 ms, 41.99% latency, 22.99 TFLOPS, \n",
      "    (0): BasicBlock(\n",
      "      3.67 M, 31.42% Params, 18.0 GMACs, 9.88% MACs, 742.67 us, 8.71% latency, 48.48 TFLOPS, \n",
      "      (conv1): Conv2d(1.18 M, 10.09% Params, 5.78 GMACs, 3.17% MACs, 148.06 us, 1.74% latency, 78.08 TFLOPS, 256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 k, 0.01% Params, 5.02 MMACs, 0.00% MACs, 51.5 us, 0.60% latency, 194.86 GFLOPS, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.00% Params, 5.02 MMACs, 0.00% MACs, 49.83 us, 0.58% latency, 201.39 GFLOPS, inplace=True)\n",
      "      (conv2): Conv2d(2.36 M, 20.18% Params, 11.56 GMACs, 6.34% MACs, 122.07 us, 1.43% latency, 189.41 TFLOPS, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 k, 0.01% Params, 5.02 MMACs, 0.00% MACs, 47.45 us, 0.56% latency, 211.51 GFLOPS, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        132.1 k, 1.13% Params, 647.27 MMACs, 0.36% MACs, 246.05 us, 2.89% latency, 5.26 TFLOPS, \n",
      "        (0): Conv2d(131.07 k, 1.12% Params, 642.25 MMACs, 0.35% MACs, 173.09 us, 2.03% latency, 7.42 TFLOPS, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1.02 k, 0.01% Params, 5.02 MMACs, 0.00% MACs, 50.78 us, 0.60% latency, 197.61 GFLOPS, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      4.72 M, 40.38% Params, 23.14 GMACs, 12.70% MACs, 2.82 ms, 33.06% latency, 16.42 TFLOPS, \n",
      "      (conv1): Conv2d(2.36 M, 20.18% Params, 11.56 GMACs, 6.34% MACs, 586.03 us, 6.87% latency, 39.45 TFLOPS, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 k, 0.01% Params, 5.02 MMACs, 0.00% MACs, 180.96 us, 2.12% latency, 55.46 GFLOPS, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.00% Params, 5.02 MMACs, 0.00% MACs, 378.61 us, 4.44% latency, 26.51 GFLOPS, inplace=True)\n",
      "      (conv2): Conv2d(2.36 M, 20.18% Params, 11.56 GMACs, 6.34% MACs, 1.16 ms, 13.67% latency, 19.85 TFLOPS, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 k, 0.01% Params, 5.02 MMACs, 0.00% MACs, 180.24 us, 2.11% latency, 55.68 GFLOPS, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(0, 0.00% Params, 2.51 MMACs, 0.00% MACs, 169.99 us, 1.99% latency, 29.52 GFLOPS, output_size=(1, 1))\n",
      "  (fc): Linear(513.0 k, 4.39% Params, 51.2 MMACs, 0.03% MACs, 964.16 us, 11.31% latency, 106.21 GFLOPS, in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from deepspeed.profiling.flops_profiler import get_model_profile\n",
    "\n",
    "model = models.resnet18().cuda()\n",
    "\n",
    "macs, _ = get_model_profile(\n",
    "    model=model,\n",
    "    input_res=(1, 3, 224, 224),\n",
    "    print_profile=True,\n",
    "    detailed=True,\n",
    "    warm_up=10,\n",
    "    as_string=False,\n",
    "    output_file=None,\n",
    "    ignore_modules=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Benchmark in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.benchmark as benchmark\n",
    "import torchvision.models as models\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def get_upsample_module(mode, upsample, ch):\n",
    "    if mode == 'deconv':\n",
    "        return nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(ch, ch, upsample, stride=upsample, dilation=1, groups=ch, bias=False),\n",
    "            torch.nn.BatchNorm2d(ch)\n",
    "        ).cuda()\n",
    "    else:\n",
    "        return nn.Upsample(scale_factor=upsample, mode=mode).cuda()\n",
    "\n",
    "def get_downsample_module(kernel, ch):\n",
    "    return nn.Sequential(\n",
    "        torch.nn.Conv2d(ch, ch, kernel, stride=1, padding=1, dilation=1, groups=ch, bias=False),\n",
    "        torch.nn.BatchNorm2d(ch)\n",
    "    ).cuda()\n",
    "\n",
    "use_benchmark = True\n",
    "amp = True\n",
    "channel_last = True\n",
    "\n",
    "torch.backends.cudnn.benchmark = use_benchmark\n",
    "\n",
    "results = []\n",
    "\n",
    "batch_size = [8, 16, 32, 64]\n",
    "channel_size = [1024, 2048]\n",
    "image_size = [16]\n",
    "mode = ['nearest', 'conv', 'deconv', 'bilinear']\n",
    "scale_factors = [2]\n",
    "\n",
    "for b, c, n in product(batch_size, channel_size, image_size):\n",
    "    label = f'Upsample (benchmark={use_benchmark}, amp={amp}, channel_last={channel_last})'\n",
    "    sub_label = f'[{b}, {c}, {n}, {n}]'\n",
    "    x = torch.rand((b, c, n, n)).cuda()\n",
    "\n",
    "    for method, upsample in product(mode, scale_factors):\n",
    "        if method == 'conv':\n",
    "            upsample += 1\n",
    "            model = get_downsample_module(upsample, c)\n",
    "        else:\n",
    "            model = get_upsample_module(method, upsample, c)\n",
    "\n",
    "        if channel_last:\n",
    "            x = x.to(memory_format=torch.channels_last)\n",
    "            model = model.to(memory_format=torch.channels_last)\n",
    "        \n",
    "        with torch.autocast('cuda', amp), torch.no_grad():\n",
    "            results.append(benchmark.Timer(\n",
    "                stmt='model(x)',\n",
    "                setup='from __main__ import model',\n",
    "                globals={'x': x},\n",
    "                label=label,\n",
    "                sub_label=sub_label,\n",
    "                description=f\"{method}(scale={upsample})\",\n",
    "                num_threads=4\n",
    "            ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "compare = benchmark.Compare(results)\n",
    "compare.colorize()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
