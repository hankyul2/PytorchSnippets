{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to analyze, benchmark, compare in pytorch ?\n",
    "\n",
    "To devleop new model in pytorch, Analyzing existing model is mandatory work to do. But I could not find any one-in-whole packages for these purpose. To analyze model, we need to see input/output, param, flops, latency and throughput for both in model level and in layer level. I spent a lot of time googling about this, these are the tools that I decide to use for each purpose. I hope this could help you.\n",
    "\n",
    "- Want to see model's structure in clean view? use `torchsummary`\n",
    "- Want to analyze model's latency/param/flops in layer? use `deepspeed.profiling.flops_profiler.get_model_profile()`. Other library seems not support these 3 important factors.\n",
    "    - `torch.profile`: This library supports latency/param/flops/memory, but in cuda kernel operation level. It seems to good for who want to optimize module in lower level.\n",
    "    - `fvcore.profile`: This library supports flops/param only. For me, it looks like upgraded version of `torchsummary`\n",
    "\n",
    "- Want to compare simply latency with others? use `torch.utils.benchmark`.\n",
    "- Want to compare overall performance(params/flops/latency/throughput)? use `benchmark.py` in `timm` github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-146            [-1, 512, 7, 7]               0\n",
      "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-155            [-1, 512, 7, 7]               0\n",
      "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-158            [-1, 512, 7, 7]               0\n",
      "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-165            [-1, 512, 7, 7]               0\n",
      "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-168            [-1, 512, 7, 7]               0\n",
      "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
      "          Linear-174                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 25,557,032\n",
      "Trainable params: 25,557,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 286.56\n",
      "Params size (MB): 97.49\n",
      "Estimated Total Size (MB): 384.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "model = models.resnet50().cuda()\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Param/Flops/Latency in layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------- DeepSpeed Flops Profiler --------------------------\n",
      "Profile Summary at step 10:\n",
      "Notations:\n",
      "data parallel size (dp_size), model parallel size(mp_size),\n",
      "number of parameters (params), number of multiply-accumulate operations(MACs),\n",
      "number of floating point operations (flops), floating point operations per second (FLOPS),\n",
      "fwd latency (forward propagation latency), bwd latency (backward propagation latency),\n",
      "step (weights update latency), iter latency (sum of fwd, bwd and step latency)\n",
      "\n",
      "params per gpu:                                               11.69 M \n",
      "params of model = params per GPU * mp_size:                   1       \n",
      "fwd MACs per GPU:                                             1.82 G  \n",
      "fwd flops per GPU = 2 * fwd MACs per GPU:                     3.64 G  \n",
      "fwd flops of model = fwd flops per GPU * mp_size:             1       \n",
      "fwd latency:                                                  7.5 ms  \n",
      "fwd FLOPS per GPU = fwd flops per GPU / fwd latency:          485.95 GFLOPS\n",
      "\n",
      "----------------------------- Aggregated Profile per GPU -----------------------------\n",
      "Top 1 modules in terms of params, MACs or fwd latency at different model depths:\n",
      "depth 0:\n",
      "    params      - {'ResNet': '11.69 M'}\n",
      "    MACs        - {'ResNet': '1.82 GMACs'}\n",
      "    fwd latency - {'ResNet': '7.5 ms'}\n",
      "depth 1:\n",
      "    params      - {'Sequential': '11.17 M'}\n",
      "    MACs        - {'Sequential': '1.7 GMACs'}\n",
      "    fwd latency - {'Sequential': '6.55 ms'}\n",
      "depth 2:\n",
      "    params      - {'BasicBlock': '11.17 M'}\n",
      "    MACs        - {'BasicBlock': '1.7 GMACs'}\n",
      "    fwd latency - {'BasicBlock': '6.42 ms'}\n",
      "depth 3:\n",
      "    params      - {'Conv2d': '10.99 M'}\n",
      "    MACs        - {'Conv2d': '1.68 GMACs'}\n",
      "    fwd latency - {'Conv2d': '3.0 ms'}\n",
      "\n",
      "------------------------------ Detailed Profile per GPU ------------------------------\n",
      "Each module profile is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\n",
      "2. Number of floating point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\n",
      "\n",
      "ResNet(\n",
      "  11.69 M, 100.00% Params, 1.82 GMACs, 100.00% MACs, 7.5 ms, 100.00% latency, 485.95 GFLOPS, \n",
      "  (conv1): Conv2d(9.41 k, 0.08% Params, 118.01 MMACs, 6.48% MACs, 241.52 us, 3.22% latency, 977.27 GFLOPS, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(128, 0.00% Params, 1.61 MMACs, 0.09% MACs, 159.5 us, 2.13% latency, 20.13 GFLOPS, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(0, 0.00% Params, 802.82 KMACs, 0.04% MACs, 64.85 us, 0.86% latency, 24.76 GFLOPS, inplace=True)\n",
      "  (maxpool): MaxPool2d(0, 0.00% Params, 802.82 KMACs, 0.04% MACs, 75.82 us, 1.01% latency, 21.18 GFLOPS, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    147.97 k, 1.27% Params, 464.83 MMACs, 25.51% MACs, 1.37 ms, 18.26% latency, 678.96 GFLOPS, \n",
      "    (0): BasicBlock(\n",
      "      73.98 k, 0.63% Params, 232.42 MMACs, 12.75% MACs, 739.57 us, 9.86% latency, 628.51 GFLOPS, \n",
      "      (conv1): Conv2d(36.86 k, 0.32% Params, 115.61 MMACs, 6.34% MACs, 248.67 us, 3.32% latency, 929.79 GFLOPS, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, 0.00% Params, 401.41 KMACs, 0.02% MACs, 82.02 us, 1.09% latency, 9.79 GFLOPS, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.00% Params, 401.41 KMACs, 0.02% MACs, 70.81 us, 0.94% latency, 11.34 GFLOPS, inplace=True)\n",
      "      (conv2): Conv2d(36.86 k, 0.32% Params, 115.61 MMACs, 6.34% MACs, 148.3 us, 1.98% latency, 1.56 TFLOPS, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, 0.00% Params, 401.41 KMACs, 0.02% MACs, 72.0 us, 0.96% latency, 11.15 GFLOPS, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      73.98 k, 0.63% Params, 232.42 MMACs, 12.75% MACs, 591.75 us, 7.89% latency, 785.51 GFLOPS, \n",
      "      (conv1): Conv2d(36.86 k, 0.32% Params, 115.61 MMACs, 6.34% MACs, 144.24 us, 1.92% latency, 1.6 TFLOPS, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, 0.00% Params, 401.41 KMACs, 0.02% MACs, 70.81 us, 0.94% latency, 11.34 GFLOPS, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.00% Params, 401.41 KMACs, 0.02% MACs, 64.37 us, 0.86% latency, 12.47 GFLOPS, inplace=True)\n",
      "      (conv2): Conv2d(36.86 k, 0.32% Params, 115.61 MMACs, 6.34% MACs, 137.33 us, 1.83% latency, 1.68 TFLOPS, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, 0.00% Params, 401.41 KMACs, 0.02% MACs, 71.76 us, 0.96% latency, 11.19 GFLOPS, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    525.57 k, 4.50% Params, 412.45 MMACs, 22.63% MACs, 1.69 ms, 22.57% latency, 487.3 GFLOPS, \n",
      "    (0): BasicBlock(\n",
      "      230.14 k, 1.97% Params, 180.63 MMACs, 9.91% MACs, 995.4 us, 13.27% latency, 362.94 GFLOPS, \n",
      "      (conv1): Conv2d(73.73 k, 0.63% Params, 57.8 MMACs, 3.17% MACs, 204.56 us, 2.73% latency, 565.13 GFLOPS, 64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, 0.00% Params, 200.7 KMACs, 0.01% MACs, 78.44 us, 1.05% latency, 5.12 GFLOPS, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.00% Params, 200.7 KMACs, 0.01% MACs, 70.1 us, 0.93% latency, 5.73 GFLOPS, inplace=True)\n",
      "      (conv2): Conv2d(147.46 k, 1.26% Params, 115.61 MMACs, 6.34% MACs, 176.19 us, 2.35% latency, 1.31 TFLOPS, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, 0.00% Params, 200.7 KMACs, 0.01% MACs, 71.05 us, 0.95% latency, 5.65 GFLOPS, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        8.45 k, 0.07% Params, 6.62 MMACs, 0.36% MACs, 272.75 us, 3.64% latency, 48.57 GFLOPS, \n",
      "        (0): Conv2d(8.19 k, 0.07% Params, 6.42 MMACs, 0.35% MACs, 166.18 us, 2.22% latency, 77.3 GFLOPS, 64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, 0.00% Params, 200.7 KMACs, 0.01% MACs, 70.81 us, 0.94% latency, 5.67 GFLOPS, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      295.42 k, 2.53% Params, 231.81 MMACs, 12.72% MACs, 663.04 us, 8.84% latency, 699.24 GFLOPS, \n",
      "      (conv1): Conv2d(147.46 k, 1.26% Params, 115.61 MMACs, 6.34% MACs, 160.93 us, 2.15% latency, 1.44 TFLOPS, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, 0.00% Params, 200.7 KMACs, 0.01% MACs, 70.1 us, 0.93% latency, 5.73 GFLOPS, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.00% Params, 200.7 KMACs, 0.01% MACs, 68.66 us, 0.92% latency, 5.85 GFLOPS, inplace=True)\n",
      "      (conv2): Conv2d(147.46 k, 1.26% Params, 115.61 MMACs, 6.34% MACs, 171.18 us, 2.28% latency, 1.35 TFLOPS, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, 0.00% Params, 200.7 KMACs, 0.01% MACs, 83.45 us, 1.11% latency, 4.81 GFLOPS, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    2.1 M, 17.96% Params, 411.74 MMACs, 22.60% MACs, 1.74 ms, 23.26% latency, 472.18 GFLOPS, \n",
      "    (0): BasicBlock(\n",
      "      919.04 k, 7.86% Params, 180.23 MMACs, 9.89% MACs, 1.01 ms, 13.52% latency, 355.57 GFLOPS, \n",
      "      (conv1): Conv2d(294.91 k, 2.52% Params, 57.8 MMACs, 3.17% MACs, 172.14 us, 2.30% latency, 671.59 GFLOPS, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, 0.00% Params, 100.35 KMACs, 0.01% MACs, 72.72 us, 0.97% latency, 2.76 GFLOPS, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.00% Params, 100.35 KMACs, 0.01% MACs, 69.38 us, 0.93% latency, 2.89 GFLOPS, inplace=True)\n",
      "      (conv2): Conv2d(589.82 k, 5.05% Params, 115.61 MMACs, 6.34% MACs, 231.03 us, 3.08% latency, 1.0 TFLOPS, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, 0.00% Params, 100.35 KMACs, 0.01% MACs, 73.43 us, 0.98% latency, 2.73 GFLOPS, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        33.28 k, 0.28% Params, 6.52 MMACs, 0.36% MACs, 272.99 us, 3.64% latency, 47.79 GFLOPS, \n",
      "        (0): Conv2d(32.77 k, 0.28% Params, 6.42 MMACs, 0.35% MACs, 145.91 us, 1.95% latency, 88.03 GFLOPS, 128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, 0.00% Params, 100.35 KMACs, 0.01% MACs, 72.24 us, 0.96% latency, 2.78 GFLOPS, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      1.18 M, 10.10% Params, 231.51 MMACs, 12.71% MACs, 698.09 us, 9.31% latency, 663.27 GFLOPS, \n",
      "      (conv1): Conv2d(589.82 k, 5.05% Params, 115.61 MMACs, 6.34% MACs, 198.13 us, 2.64% latency, 1.17 TFLOPS, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, 0.00% Params, 100.35 KMACs, 0.01% MACs, 75.82 us, 1.01% latency, 2.65 GFLOPS, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.00% Params, 100.35 KMACs, 0.01% MACs, 74.39 us, 0.99% latency, 2.7 GFLOPS, inplace=True)\n",
      "      (conv2): Conv2d(589.82 k, 5.05% Params, 115.61 MMACs, 6.34% MACs, 173.09 us, 2.31% latency, 1.34 TFLOPS, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, 0.00% Params, 100.35 KMACs, 0.01% MACs, 72.24 us, 0.96% latency, 2.78 GFLOPS, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    8.39 M, 71.81% Params, 411.39 MMACs, 22.58% MACs, 1.75 ms, 23.28% latency, 471.19 GFLOPS, \n",
      "    (0): BasicBlock(\n",
      "      3.67 M, 31.42% Params, 180.03 MMACs, 9.88% MACs, 991.82 us, 13.23% latency, 363.03 GFLOPS, \n",
      "      (conv1): Conv2d(1.18 M, 10.09% Params, 57.8 MMACs, 3.17% MACs, 172.38 us, 2.30% latency, 670.66 GFLOPS, 256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 k, 0.01% Params, 50.18 KMACs, 0.00% MACs, 71.76 us, 0.96% latency, 1.4 GFLOPS, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.00% Params, 50.18 KMACs, 0.00% MACs, 68.43 us, 0.91% latency, 1.47 GFLOPS, inplace=True)\n",
      "      (conv2): Conv2d(2.36 M, 20.18% Params, 115.61 MMACs, 6.34% MACs, 246.76 us, 3.29% latency, 936.98 GFLOPS, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 k, 0.01% Params, 50.18 KMACs, 0.00% MACs, 74.39 us, 0.99% latency, 1.35 GFLOPS, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        132.1 k, 1.13% Params, 6.47 MMACs, 0.36% MACs, 236.03 us, 3.15% latency, 54.85 GFLOPS, \n",
      "        (0): Conv2d(131.07 k, 1.12% Params, 6.42 MMACs, 0.35% MACs, 136.61 us, 1.82% latency, 94.02 GFLOPS, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1.02 k, 0.01% Params, 50.18 KMACs, 0.00% MACs, 66.76 us, 0.89% latency, 1.5 GFLOPS, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      4.72 M, 40.38% Params, 231.36 MMACs, 12.70% MACs, 723.84 us, 9.65% latency, 639.26 GFLOPS, \n",
      "      (conv1): Conv2d(2.36 M, 20.18% Params, 115.61 MMACs, 6.34% MACs, 218.63 us, 2.92% latency, 1.06 TFLOPS, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 k, 0.01% Params, 50.18 KMACs, 0.00% MACs, 73.67 us, 0.98% latency, 1.36 GFLOPS, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.00% Params, 50.18 KMACs, 0.00% MACs, 65.8 us, 0.88% latency, 1.53 GFLOPS, inplace=True)\n",
      "      (conv2): Conv2d(2.36 M, 20.18% Params, 115.61 MMACs, 6.34% MACs, 191.69 us, 2.56% latency, 1.21 TFLOPS, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 k, 0.01% Params, 50.18 KMACs, 0.00% MACs, 71.29 us, 0.95% latency, 1.41 GFLOPS, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(0, 0.00% Params, 25.09 KMACs, 0.00% MACs, 77.49 us, 1.03% latency, 647.55 MFLOPS, output_size=(1, 1))\n",
      "  (fc): Linear(513.0 k, 4.39% Params, 512.0 KMACs, 0.03% MACs, 151.63 us, 2.02% latency, 6.75 GFLOPS, in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from deepspeed.profiling.flops_profiler import get_model_profile\n",
    "\n",
    "model = models.resnet18().cuda()\n",
    "\n",
    "macs, _ = get_model_profile(\n",
    "    model=model,\n",
    "    input_res=(1, 3, 224, 224),\n",
    "    print_profile=True,\n",
    "    detailed=True,\n",
    "    warm_up=10,\n",
    "    as_string=False,\n",
    "    output_file=None,\n",
    "    ignore_modules=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Benchmark in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.benchmark as benchmark\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def batched_dot_mul_sum(a, b):\n",
    "    return a.mul(b).sum(-1)\n",
    "\n",
    "def batched_dot_bmm(a, b):\n",
    "    a = a.unsqueeze(1)\n",
    "    b = b.unsqueeze(-1)\n",
    "    return a.bmm(b).flatten(-3)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = torch.rand([10, 64])\n",
    "    assert batched_dot_mul_sum(x, x).allclose(batched_dot_bmm(x, x))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    size = [1, 64, 256]\n",
    "\n",
    "    for b, n in product(size, size):\n",
    "        label = 'matrix multiplication'\n",
    "        sub_label = f'[{b}, {n}]'\n",
    "        x = torch.rand((b, n))\n",
    "\n",
    "        for num_threads in [1, 4, 8, 16]:\n",
    "            results.append(benchmark.Timer(\n",
    "                stmt='batched_dot_mul_sum(x, x)',\n",
    "                setup='from __main__ import batched_dot_mul_sum',\n",
    "                globals={'x': x},\n",
    "                label=label,\n",
    "                sub_label=sub_label,\n",
    "                description='mul',\n",
    "                num_threads=num_threads\n",
    "            ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "            results.append(benchmark.Timer(\n",
    "                stmt='batched_dot_bmm(x, x)',\n",
    "                setup='from __main__ import batched_dot_bmm',\n",
    "                globals={'x': x},\n",
    "                label=label,\n",
    "                sub_label=sub_label,\n",
    "                description='bmm',\n",
    "                num_threads=num_threads\n",
    "            ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "    compare = benchmark.Compare(results)\n",
    "    compare.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
